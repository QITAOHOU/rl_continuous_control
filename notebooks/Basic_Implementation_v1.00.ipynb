{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from agents.DDPG import DDPG\n",
    "from agents.DDPGplus import DDPGplus\n",
    "from agents.D4PG import D4PG\n",
    "\n",
    "from util import *\n",
    "\n",
    "PATH = \"/Volumes/BC_Clutch/Dropbox/Programming/Classes/Udacity/DeepRLND/rl_continuous_control/\"\n",
    "\n",
    "env_dict = {\n",
    "            \"Reacher1\":\"Reacher1.app\",\n",
    "            \"Reacher20\":\"Reacher20.app\"\n",
    "            }\n",
    "\n",
    "agent_dict = {\n",
    "              \"DDPG\":DDPG, \n",
    "#               \"DDPGplus\":DDPGplus,\n",
    "#               \"D4PG\":D4PG\n",
    "              }\n",
    "\n",
    "result_dict = {}\n",
    "for ke, ve in env_dict.items():\n",
    "    env_name = ke\n",
    "    fp = PATH + f\"data/{ve}\"\n",
    "    env = UnityEnvironment(file_name=fp)\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    action_size = brain.vector_action_space_size\n",
    "    state_size = env_info.vector_observations.shape[1]\n",
    "    for ka, va in agent_dict.items():\n",
    "        start = time.time()\n",
    "        total_scores = []\n",
    "        agent_name = ka\n",
    "        print(f\"Agent: {agent_name}\")\n",
    "        agent = va(state_size, action_size, num_agents)\n",
    "        for i in range(1,10000):\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "            scores = np.zeros(num_agents)\n",
    "            agent.reset()\n",
    "            for t in range(1000):\n",
    "                actions = agent.act(states)\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "                agent.step(states, actions, rewards, next_states, dones, t)\n",
    "                states = next_states\n",
    "                scores += env_info.rewards\n",
    "                if np.any(dones):\n",
    "                    break\n",
    "            length = min(100, len(scores))\n",
    "            mean_score = np.mean(scores)\n",
    "            total_scores.append(mean_score)\n",
    "            total_mean_score = np.mean(total_scores[-length:])\n",
    "            print(f\"\\rEpisode: {i}\\tScore: {mean_score:.2f}\")\n",
    "            if total_mean_score>0.05:\n",
    "                print(f\"Solved in {i} episodes.\")\n",
    "                break\n",
    "        end = time.time()\n",
    "        result_dict[(env_name, agent_name)] = {\n",
    "                        \"Scores\": total_scores,\n",
    "                        \"Runtime\": calc_runtime(end-start)\n",
    "                        }\n",
    "    env.close()\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = {\n",
    "            \"Reacher1\":\"Reacher1.app\",\n",
    "            \"Reacher20\":\"Reacher20.app\"\n",
    "            }\n",
    "\n",
    "agent_dict = {\n",
    "              \"DDPG\":DDPG, \n",
    "#               \"DDPGplus\":DDPGplus,\n",
    "#               \"D4PG\":D4PG\n",
    "              }\n",
    "\n",
    "result_dict = {}\n",
    "for ke, ve in env_dict.items():\n",
    "    env_name = ke\n",
    "    fp = PATH + f\"data/{ve}\"\n",
    "    env = UnityEnvironment(file_name=fp)\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    action_size = brain.vector_action_space_size\n",
    "    state_size = env_info.vector_observations.shape[1]\n",
    "    for ka, va in agent_dict.items():\n",
    "        start = time.time()\n",
    "        total_scores = []\n",
    "        agent_name = ka\n",
    "        print(f\"Agent: {agent_name}\")\n",
    "        agent = va(state_size, action_size, num_agents)\n",
    "        for i in range(1,10000):\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "            scores = np.zeros(num_agents)\n",
    "            agent.reset()\n",
    "            for t in range(1000):\n",
    "                actions = agent.act(states)\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "                agent.step(states, actions, rewards, next_states, dones, t)\n",
    "                states = next_states\n",
    "                scores += env_info.rewards\n",
    "                if np.any(dones):\n",
    "                    break\n",
    "            length = min(100, len(scores))\n",
    "            mean_score = np.mean(scores)\n",
    "            total_scores.append(mean_score)\n",
    "            total_mean_score = np.mean(total_scores[-length:])\n",
    "            print(f\"\\rEpisode: {i}\\tScore: {mean_score:.2f}\")\n",
    "            if total_mean_score>0.05:\n",
    "                print(f\"Solved in {i} episodes.\")\n",
    "                break\n",
    "        end = time.time()\n",
    "        result_dict[(env_name, agent_name)] = {\n",
    "                        \"Scores\": total_scores,\n",
    "                        \"Runtime\": calc_runtime(end-start)\n",
    "                        }\n",
    "    env.close()\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from agents.DDPG import DDPG\n",
    "\n",
    "from util import *\n",
    "\n",
    "PATH = \"/Volumes/BC_Clutch/Dropbox/Programming/Classes/Udacity/DeepRLND/rl_continuous_control/\"\n",
    "\n",
    "env_dict = {\n",
    "            \"Reacher1\":\"Reacher1.app\",\n",
    "            \"Reacher20\":\"Reacher20.app\"\n",
    "            }\n",
    "\n",
    "result_dict = {}\n",
    "for ke, ve in env_dict.items():\n",
    "    start = time.time()\n",
    "    total_scores = []\n",
    "    env_name = ke\n",
    "    print(f\"Environment: {env_name}\")\n",
    "    fp = PATH + f\"data/{ve}\"\n",
    "    env = UnityEnvironment(file_name=fp)\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    action_size = brain.vector_action_space_size\n",
    "    state_size = env_info.vector_observations.shape[1]\n",
    "    agent = DDPG(state_size, action_size, num_agents)\n",
    "    for i in range(1,10000):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "        agent.reset()\n",
    "        for t in range(1000):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "            states = next_states\n",
    "            scores += env_info.rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        length = min(100, len(scores))\n",
    "        mean_score = np.mean(scores)\n",
    "        total_scores.append(mean_score)\n",
    "        total_mean_score = np.mean(total_scores[-length:])\n",
    "        print(f\"\\rEpisode: {i}\\tScore: {mean_score:.2f}\")\n",
    "        if total_mean_score>0.05:\n",
    "            print(f\"Solved in {i} episodes.\")\n",
    "            break\n",
    "    end = time.time()\n",
    "    result_dict[env_name] = {\n",
    "                    \"Scores\": total_scores,\n",
    "                    \"Runtime\": calc_runtime(end-start)\n",
    "                    }\n",
    "#     env.close()\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\tScore: 0.00\n",
      "Episode: 2\tScore: 0.27\n",
      "Episode: 3\tScore: 0.00\n",
      "Episode: 4\tScore: 0.00\n",
      "Episode: 5\tScore: 1.75\n",
      "Episode: 6\tScore: 2.32\n",
      "Episode: 7\tScore: 0.16\n",
      "Episode: 8\tScore: 0.28\n",
      "Episode: 9\tScore: 0.78\n",
      "Episode: 10\tScore: 1.62\n",
      "Episode: 11\tScore: 1.15\n",
      "Episode: 12\tScore: 0.57\n",
      "Episode: 13\tScore: 0.61\n",
      "Episode: 14\tScore: 0.33\n",
      "Episode: 15\tScore: 1.37\n",
      "Episode: 16\tScore: 1.94\n",
      "Episode: 17\tScore: 2.63\n",
      "Episode: 18\tScore: 0.89\n",
      "Episode: 19\tScore: 2.13\n",
      "Episode: 20\tScore: 1.32\n",
      "Episode: 21\tScore: 2.23\n",
      "Episode: 22\tScore: 0.18\n",
      "Episode: 23\tScore: 1.08\n",
      "Episode: 24\tScore: 2.11\n",
      "Episode: 25\tScore: 1.69\n",
      "Episode: 26\tScore: 3.92\n",
      "Episode: 27\tScore: 3.51\n",
      "Episode: 28\tScore: 2.25\n",
      "Episode: 29\tScore: 2.61\n",
      "Episode: 30\tScore: 2.19\n",
      "Episode: 31\tScore: 1.18\n",
      "Episode: 32\tScore: 0.48\n",
      "Episode: 33\tScore: 2.41\n",
      "Episode: 34\tScore: 0.43\n",
      "Episode: 35\tScore: 1.10\n",
      "Episode: 36\tScore: 1.15\n",
      "Episode: 37\tScore: 0.69\n",
      "Episode: 38\tScore: 0.25\n",
      "Episode: 39\tScore: 0.57\n",
      "Episode: 40\tScore: 1.61\n",
      "Episode: 41\tScore: 1.56\n",
      "Episode: 42\tScore: 1.98\n",
      "Episode: 43\tScore: 1.60\n",
      "Episode: 44\tScore: 0.72\n",
      "Episode: 45\tScore: 0.62\n",
      "Episode: 46\tScore: 1.89\n",
      "Episode: 47\tScore: 0.96\n",
      "Episode: 48\tScore: 1.62\n",
      "Episode: 49\tScore: 0.79\n",
      "Episode: 50\tScore: 1.97\n",
      "Episode: 51\tScore: 0.53\n",
      "Episode: 52\tScore: 0.58\n",
      "Episode: 53\tScore: 1.33\n",
      "Episode: 54\tScore: 1.95\n",
      "Episode: 55\tScore: 1.14\n",
      "Episode: 56\tScore: 3.02\n",
      "Episode: 57\tScore: 0.60\n",
      "Episode: 58\tScore: 1.64\n",
      "Episode: 59\tScore: 1.08\n",
      "Episode: 60\tScore: 0.77\n",
      "Episode: 61\tScore: 2.30\n",
      "Episode: 62\tScore: 2.25\n",
      "Episode: 63\tScore: 2.28\n",
      "Episode: 64\tScore: 2.47\n",
      "Episode: 65\tScore: 1.45\n",
      "Episode: 66\tScore: 1.07\n",
      "Episode: 67\tScore: 0.65\n",
      "Episode: 68\tScore: 1.37\n",
      "Episode: 69\tScore: 1.25\n",
      "Episode: 70\tScore: 1.60\n",
      "Episode: 71\tScore: 2.10\n",
      "Episode: 72\tScore: 2.29\n",
      "Episode: 73\tScore: 0.58\n",
      "Episode: 74\tScore: 0.85\n",
      "Episode: 75\tScore: 0.14\n",
      "Episode: 76\tScore: 1.46\n",
      "Episode: 77\tScore: 0.24\n",
      "Episode: 78\tScore: 1.47\n",
      "Episode: 79\tScore: 2.02\n",
      "Episode: 80\tScore: 1.65\n",
      "Episode: 81\tScore: 0.82\n",
      "Episode: 82\tScore: 1.41\n",
      "Episode: 83\tScore: 0.79\n",
      "Episode: 84\tScore: 0.59\n",
      "Episode: 85\tScore: 1.18\n",
      "Episode: 86\tScore: 1.41\n",
      "Episode: 87\tScore: 0.43\n",
      "Episode: 88\tScore: 0.89\n",
      "Episode: 89\tScore: 0.24\n",
      "Episode: 90\tScore: 0.35\n",
      "Episode: 91\tScore: 1.22\n",
      "Episode: 92\tScore: 0.44\n",
      "Episode: 93\tScore: 0.00\n",
      "Episode: 94\tScore: 0.23\n",
      "Episode: 95\tScore: 0.39\n",
      "Episode: 96\tScore: 3.15\n",
      "Episode: 97\tScore: 0.11\n",
      "Episode: 98\tScore: 0.48\n",
      "Episode: 99\tScore: 0.31\n",
      "Episode: 100\tScore: 1.62\n",
      "Episode: 101\tScore: 1.65\n",
      "Episode: 102\tScore: 1.28\n",
      "Episode: 103\tScore: 0.60\n",
      "Episode: 104\tScore: 1.68\n",
      "Episode: 105\tScore: 0.40\n",
      "Episode: 106\tScore: 0.17\n",
      "Episode: 107\tScore: 1.89\n",
      "Episode: 108\tScore: 1.74\n",
      "Episode: 109\tScore: 1.57\n",
      "Episode: 110\tScore: 0.95\n",
      "Episode: 111\tScore: 2.87\n",
      "Episode: 112\tScore: 1.51\n",
      "Episode: 113\tScore: 1.09\n",
      "Episode: 114\tScore: 1.17\n",
      "Episode: 115\tScore: 1.19\n",
      "Episode: 116\tScore: 0.63\n",
      "Episode: 117\tScore: 1.56\n",
      "Episode: 118\tScore: 1.34\n",
      "Episode: 119\tScore: 1.11\n",
      "Episode: 120\tScore: 2.13\n",
      "Episode: 121\tScore: 0.38\n",
      "Episode: 122\tScore: 1.37\n",
      "Episode: 123\tScore: 0.91\n",
      "Episode: 124\tScore: 2.29\n",
      "Episode: 125\tScore: 0.83\n",
      "Episode: 126\tScore: 0.74\n",
      "Episode: 127\tScore: 1.09\n",
      "Episode: 128\tScore: 1.58\n",
      "Episode: 129\tScore: 0.52\n",
      "Episode: 130\tScore: 1.21\n",
      "Episode: 131\tScore: 0.89\n",
      "Episode: 132\tScore: 0.34\n",
      "Episode: 133\tScore: 0.13\n",
      "Episode: 134\tScore: 1.27\n",
      "Episode: 135\tScore: 0.98\n",
      "Episode: 136\tScore: 1.05\n",
      "Episode: 137\tScore: 0.62\n",
      "Episode: 138\tScore: 1.16\n",
      "Episode: 139\tScore: 0.56\n",
      "Episode: 140\tScore: 1.61\n",
      "Episode: 141\tScore: 1.68\n",
      "Episode: 142\tScore: 0.00\n",
      "Episode: 143\tScore: 0.85\n",
      "Episode: 144\tScore: 0.35\n",
      "Episode: 145\tScore: 0.27\n",
      "Episode: 146\tScore: 0.15\n",
      "Episode: 147\tScore: 0.59\n",
      "Episode: 148\tScore: 0.92\n",
      "Episode: 149\tScore: 2.11\n",
      "Episode: 150\tScore: 2.46\n",
      "Episode: 151\tScore: 0.32\n",
      "Episode: 152\tScore: 0.26\n",
      "Episode: 153\tScore: 0.00\n",
      "Episode: 154\tScore: 0.47\n",
      "Episode: 155\tScore: 0.00\n",
      "Episode: 156\tScore: 0.00\n",
      "Episode: 157\tScore: 0.00\n",
      "Episode: 158\tScore: 0.28\n",
      "Episode: 159\tScore: 0.22\n",
      "Episode: 160\tScore: 0.24\n",
      "Episode: 161\tScore: 0.47\n",
      "Episode: 162\tScore: 1.42\n",
      "Episode: 163\tScore: 2.20\n",
      "Episode: 164\tScore: 0.21\n",
      "Episode: 165\tScore: 1.00\n",
      "Episode: 166\tScore: 0.64\n",
      "Episode: 167\tScore: 0.43\n",
      "Episode: 168\tScore: 0.60\n",
      "Episode: 169\tScore: 2.85\n",
      "Episode: 170\tScore: 1.26\n",
      "Episode: 171\tScore: 0.93\n",
      "Episode: 172\tScore: 0.35\n",
      "Episode: 173\tScore: 0.63\n",
      "Episode: 174\tScore: 0.14\n",
      "Episode: 175\tScore: 1.71\n",
      "Episode: 176\tScore: 1.35\n",
      "Episode: 177\tScore: 0.51\n",
      "Episode: 178\tScore: 0.26\n",
      "Episode: 179\tScore: 0.42\n",
      "Episode: 180\tScore: 1.90\n",
      "Episode: 181\tScore: 0.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5516f6be8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/BC_Clutch/Dropbox/Programming/Classes/Udacity/DeepRLND/rl_continuous_control/notebooks/agents/DDPG.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, timestep, alpha, beta)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_learn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/BC_Clutch/Dropbox/Programming/Classes/Udacity/DeepRLND/rl_continuous_control/notebooks/agents/DDPG.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from agents.DDPG import DDPG\n",
    "from agents.DDPGplus import DDPGplus\n",
    "from agents.D4PG import D4PG\n",
    "\n",
    "from util import *\n",
    "\n",
    "PATH = \"/Volumes/BC_Clutch/Dropbox/Programming/Classes/Udacity/DeepRLND/rl_continuous_control/\"\n",
    "\n",
    "agent_dict = {\n",
    "              \"DDPG\":DDPG, \n",
    "              \"DDPGplus\":DDPGplus,\n",
    "              \"D4PG\":D4PG\n",
    "              }\n",
    "\n",
    "result_dict = {}\n",
    "start = time.time()\n",
    "total_scores = []\n",
    "env_name = \"Reacher1\"\n",
    "fp = PATH + f\"data/Reacher1.app\"\n",
    "env = UnityEnvironment(file_name=fp)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "for k, v in agent_dict.items():\n",
    "    agent_name = k\n",
    "    agent = v(state_size, action_size, num_agents)\n",
    "    for i in range(1,10000):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "        agent.reset()\n",
    "        for t in range(1000):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "            states = next_states\n",
    "            scores += env_info.rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        length = min(100, len(scores))\n",
    "        mean_score = np.mean(scores)\n",
    "        total_scores.append(mean_score)\n",
    "        total_mean_score = np.mean(total_scores[-length:])\n",
    "        print(f\"\\rEpisode: {i}\\tScore: {mean_score:.2f}\")\n",
    "        if total_mean_score>30.0:\n",
    "            print(f\"Solved in {i} episodes.\")\n",
    "            break\n",
    "    end = time.time()\n",
    "    result_dict[agent_name] = {\n",
    "                    \"Scores\": total_scores,\n",
    "                    \"Runtime\": calc_runtime(end-start)\n",
    "                    }\n",
    "#     env.close()\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd 2",
   "language": "python",
   "name": "drlnd2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
